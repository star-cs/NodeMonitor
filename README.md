# NodeMonitor
高性能系统监控解决方案，采用C++开发，基于CMake构建系统，集成Protocol Buffers和gRPC实现高效数据序列化与远程过程调用。本项目创新性地结合了eBPF和mmap技术，实现了高性能的系统监控数据采集：通过eBPF技术进行网络流量和系统事件的高效捕获与分析，通过mmap机制实现内核与用户空间的数据高效共享，大幅提升监控性能。  

项目专注于节点级系统监控，能够实时采集CPU（负载、软中断、状态）、内存、网络、磁盘等关键性能指标，并通过客户端-服务器架构提供灵活的数据访问接口。项目包含监控数据展示组件、RPC通信管理模块及测试框架，支持Docker容器化部署，适用于高性能计算环境和分布式系统监控场景。

# 开发环境 
```bash
cd ./docker/build

docker build --network host  -t linux:monitor -f base.dockerfile .

bash docker/scripts/monitor_docker_run.sh
bash docker/scripts/monitor_docker_into.sh
```

# 监控指标
## CPU负载

平均负载是指单位时间内平均活跃进程数，它包括了系统处于可运行状态和不可中断状态的进程数。不仅包括了正在使用CPU的进程，还包括等待CPU和等待I/O的进程。

平均负载最理想的情况就说等于CPU个数，超过CPU个数表示系统出现了过载

分析：
- 如果 1，5，15分钟的平均负载相差不大，说明系统负载很平稳
- 如果 1分钟 远小于 15分钟，说明最近1分钟的负载在降低，而过去15分钟内有很大的负载。可能需要检查 14分钟内 哪些进程导致了 负载升高
- 相反，如果 1分钟 远大于 15分钟，说明最近1分钟的负载在增加，可能是临时性的，也可能会持续增加。需要持续观察
- 一旦 1分钟平均负载接近 或 超过了 CPU个数，以为系统发送了过载，需要分析哪里导致的问题。

平均负载高 的 原因：CPU密集型进程导致，I/O繁忙

## CPU软中断

## CPU state

## disk
/proc/diskstats

## mem
/proc/meminfo

## net
bepf


# 采集方法 
## /proc
/proc低频数据，内核维护的数据本身的变化频率低（内存、负载），直接读取/proc文件系统的方式，获取内核导出的统计信息。/proc是内核官方的接口，兼容性好、实现简单，直接解析其中的文本就可以。采集延迟的性能也足够。通常50-100ms。更适合小规模数据

- 缺点：通过read()读取/proc文件的方式获取，内核都要把原始二进制数据格式化为文本，用户空间还要再解析文本为结构体或数值，增加了 CPU 开销。特别是内核要把数据内容拷贝到用户缓冲区，对于监控采集频率较高的情况，拷贝开销高。

每5秒更新一次，采集的实时性要求不高。  
几乎所有的主流linux性能工具都基于 /proc 实现：
- top/htop:通过解析/proc/stat，/proc/[pid]/stat 等文件获取CPU和进程信息
- free:直接读取/proc/meminfo显示内存使用情况
- ps:扫描/proc/[pid]/目录下的各种状态文件

## 内核模块mmap
[从内核世界透视 mmap 内存映射的本质（原理篇）](https://www.cnblogs.com/binlovetech/p/17712761.html)

mmap高频数据，内核维护的数据本身的变化频率高（cpu软中断，cpu使用），采集逻辑是通过自定义实现内核模块（直接采集内核内部的数据，通过 mmap 机制将内核内存映射到用户空间）。
1. 根据监控需求定义合适的数据结构，并在内核分配一块内存，存储原始数据，作为内核和用户空间共享的数据区。
2. 再通过定时器实现每秒定时获取。
3. 接下来暴露用户进程，将内核模块注册为字符设备，并实现 mmap 回调，这样用户空间可以通过 mmap 将内核分配的内存区映射到用户空间。
4. 之后首次访问该虚拟地址时，操作系统通过缺页异常，将这段虚拟地址和内核的物理地址建立映射关系（页表项），之后无需再次拷贝，用户就可以直接访问映射区内存实时获取到最新的内核数据。户空间访问的就是内核分配的真实物理内存的映射。实现用户与内核的高效实时交互。

- 要实现更低的性能开销和低延迟，通常延迟1-5ms
- mmap内存映射避免内核态到用户态的数据拷贝还有文本格式化的过程，零拷贝，开销极低，最大限度降低采集延迟，性能特别高，几乎没有额外的开销，非常适合高频采集、实时大数据量的监控场景。
- 相比eBPF少一层虚拟机开销
- 缺点：
    - 需要内核编程，开发难度高
    - 存在安全风险：内核模块可以按照需要加载和卸载扩展内核的功能，但这是完全的内核编程，存在安全性的问题，如果内核代码崩溃，整台机器及其上运行的所有程序都会崩溃。因为内核是特权代码，它可以访问机器上的所有内容，包括所有数据，因此内核中的恶意代码将是一个严重的问题。

## ebpf
eBPF程序eBPF 程序运行在内核的受限虚拟机中，可以动态挂载到内核的网络协议栈的 hook 点（系统调用等），比如这里TC Hook在协议栈入口抓包，延迟<1ms，在内核态实时采集和处理数据，并通过 eBPF map 与用户空间交互获取数据。eBPF 可以直接挂载到网络协议栈的关键路径（如 TC 或 XDP），对每个经过的数据包进行实时统计和处理
- 能在不更改内核代码的前提下，实时获取和修改操作系统的行为。像插件一样扩展内核功能
- eBPF 程序可以在系统运行时随时加载和卸载，不需要重启内核或影响现有业务。。
- 安全性好，eBPF 运行在受限的沙箱环境，而且eBPF 验证器，确保只有在安全运行的情况下才能加载 eBPF 程序——它不会导致机器崩溃或陷入死循环，也不会允许数据被泄露。
- 适合网络流量、包级别的实时监控，能做到极低的性能开销和高频低延迟采集。
- bcc、libbpf、bpftrace 等工具链完善，开发和集成门槛低，社区有大量成熟案例可借鉴。
- 缺点：BPF 程序虽然运行在内核态，但是在一个受限的虚拟机中执行，所有指令都要经过验证和解释，相比直接用内核模块访问内核数据结构，存在一定的虚拟机开销，性能略低于内核模块mmap

### TC 和 XDP
- 都是 Linux 内核里用来处理网络数据包的技术
- TC（Traffic Control）
    - TC 是 Linux 网络协议栈中的一个流量控制框架，主要用来做流量整形、限速、包过滤等。
    - TC 的 hook 点位于网络协议栈和网卡驱动之间的“中间层”。也就是数据包已经经过内核协议处理，但还没在出方向真正发到网卡或者在入方向刚从网卡收到。
        - 入方向（ingress）：数据包从网卡收到后，先经过 TC ingress，再进入内核协议栈（如 IP 层、TCP/UDP 层等）。
        - 出方向（egress）：数据包在内核协议栈里被应用或系统生成后，先经过 TC egress，再发到网卡驱动，最终通过物理网卡发送出去。
    - 用 TC 挂载 eBPF 程序，可以对进出网卡的数据包做各种统计、过滤和处理，适合需要更多协议信息和上下文的场景。
    - 网络流量统计可以用 TC
- XDP（eXpress Data Path）
    - XDP 是 Linux 内核里专门为高性能网络处理设计的一个框架。
    - 它的 hook 点在网卡驱动的最前面，数据包刚到网卡、还没进入内核协议栈时就能被处理。
    - 用 XDP 挂载 eBPF 程序，可以极快地处理、丢弃、转发数据包，延迟极低，非常适合高性能、低延迟的网络监控和防护场景。
    - 需要极致的性能可以用 XDP

## 系统调用
比如通过 sysinfo、getrusage、ioctl 等系统调用来获取系统资源信息，这种方式确实可行，很多基础的监控工具也会用到。
- 但系统调用的方式通常功能有限，有些只能获取到部分的全局资源信息，统计数据不全面，比如整体的内存、CPU 使用率等，无法细致到每个 CPU、每块网卡、每类软中断等详细指标。满足不了监控的数据需求，而且本质上也还是基于读取/proc文件或者内核的变量
    - sysinfo获取系统整体的内存、交换空间、负载等全局信息
    - getrusage获取当前进程或其子进程的资源使用情况